{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP96K40h3Jo8B3dGs7gDK4Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"anvoble8bw0d","executionInfo":{"status":"ok","timestamp":1687006711572,"user_tz":-60,"elapsed":8742,"user":{"displayName":"Ashwin Kashyap","userId":"16952148365127114775"}}},"outputs":[],"source":["import torch\n","import torchvision.transforms as T\n","import cv2\n","from PIL import Image"]},{"cell_type":"code","source":["dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YqF4poG3b1dD","executionInfo":{"status":"ok","timestamp":1687006715995,"user_tz":-60,"elapsed":2671,"user":{"displayName":"Ashwin Kashyap","userId":"16952148365127114775"}},"outputId":"0c924bc6-9587-4d98-f2f0-8d2c4c1e20e0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n","WARNING:dinov2:xFormers not available\n","WARNING:dinov2:xFormers not available\n","Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vits14_pretrain.pth\n","100%|██████████| 84.2M/84.2M [00:00<00:00, 180MB/s]\n"]}]},{"cell_type":"code","source":["transform = T.Compose([\n","    T.Resize((224, 224)),  # Resize the image to the model's input size\n","    T.ToTensor(),  # Convert the image to a tensor\n","    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize the image\n","])"],"metadata":{"id":"7RoTQm-tb5Om","executionInfo":{"status":"ok","timestamp":1687006724802,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ashwin Kashyap","userId":"16952148365127114775"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def process_frame(frame):\n","    # Convert the frame to PIL Image\n","    image = Image.fromarray(frame)\n","\n","    # Apply the image transformations\n","    image = transform(image).unsqueeze(0)  # Add a batch dimension\n","\n","    # Extract features from the model\n","    with torch.no_grad():\n","        features = dinov2_vits14.forward_features(image)\n","\n","    # Do further processing or perform desired tasks with the extracted features\n","\n","    # Return the processed frame\n","    return frame\n","\n","camera = cv2.VideoCapture(0)  # 0 indicates the default camera\n","\n","while True:\n","    ret, frame = camera.read()  # Read a frame from the camera\n","\n","    processed_frame = process_frame(frame)  # Process the frame\n","\n","    cv2.imshow('Camera', processed_frame)  # Display the processed frame\n","\n","    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit the loop\n","        break\n","\n","camera.release()  # Release the camera\n","cv2.destroyAllWindows()  # Close the windows"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":375},"id":"57rRsYPFb8CX","executionInfo":{"status":"error","timestamp":1687006776303,"user_tz":-60,"elapsed":224,"user":{"displayName":"Ashwin Kashyap","userId":"16952148365127114775"}},"outputId":"0355505c-8c89-490f-e1a0-5c6aec823e4d"},"execution_count":6,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-de210a23596a>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcamera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Read a frame from the camera\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mprocessed_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Process the frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Camera'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_frame\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Display the processed frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-de210a23596a>\u001b[0m in \u001b[0;36mprocess_frame\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Convert the frame to PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Apply the image transformations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2823\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;36m.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2824\u001b[0m     \"\"\"\n\u001b[0;32m-> 2825\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_interface__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2826\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '__array_interface__'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"O7LVCqeucAFC"},"execution_count":null,"outputs":[]}]}
